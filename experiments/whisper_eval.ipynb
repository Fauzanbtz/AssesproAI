{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1RGHdxTN0JY"
   },
   "source": [
    "IMPORT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-30T12:34:38.177691Z",
     "iopub.status.busy": "2025-11-30T12:34:38.177363Z",
     "iopub.status.idle": "2025-11-30T12:34:42.301008Z",
     "shell.execute_reply": "2025-11-30T12:34:42.299868Z",
     "shell.execute_reply.started": "2025-11-30T12:34:38.177669Z"
    },
    "id": "O6lboXzQN2BS",
    "outputId": "b9c81ab0-10da-458f-9dc8-7c46a058b066"
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from jiwer import wer, cer\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZw8XzyVN82H"
   },
   "source": [
    "**VIDIO 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-30T12:41:23.694385Z",
     "iopub.status.busy": "2025-11-30T12:41:23.689196Z",
     "iopub.status.idle": "2025-11-30T12:41:38.495209Z",
     "shell.execute_reply": "2025-11-30T12:41:38.491629Z",
     "shell.execute_reply.started": "2025-11-30T12:41:23.694058Z"
    },
    "id": "TYEwTkYkN43q",
    "outputId": "95e1da7c-1a48-46b9-ed22-24621425e2ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.1.1-3ubuntu5 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with gcc 13 (Ubuntu 13.2.0-23ubuntu3)\n",
      "  configuration: --prefix=/usr --extra-version=3ubuntu5 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --disable-omx --enable-gnutls --enable-libaom --enable-libass --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libharfbuzz --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-openal --enable-opencl --enable-opengl --disable-sndio --enable-libvpl --disable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-ladspa --enable-libbluray --enable-libjack --enable-libpulse --enable-librabbitmq --enable-librist --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libx264 --enable-libzmq --enable-libzvbi --enable-lv2 --enable-sdl2 --enable-libplacebo --enable-librav1e --enable-pocketsphinx --enable-librsvg --enable-libjxl --enable-shared\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "[in#0 @ 0x5617a7b839c0] Error opening input: No such file or directory\n",
      "Error opening input file /content/interview_question_1.mp4.\n",
      "Error opening input files: No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mCan you share any specific challenges you face while working on certification and how you overcome them? Ah, okay. Actually, for the challenges, there are some challenges when I took the certifications, especially for the projects I mentioned that I already working with it. The first one is actually to meet the specific accuracy or validation loss right for the evaluation matrix.\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mAnd yeah, actually, that\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms just need to take some trial and error with different architecture. For example, we can try to add more layer, more neurons, changes the neurons, or even I also apply the dropout layer. So yeah, it really helps with the validation loss to become more lower, right? And yeah, I think that\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms one of the biggest challenges that I have while working on these certifications.\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     10\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedium\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# tiny/base/small/medium/large-v2\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mload_model(model_name)\n\u001b[1;32m     13\u001b[0m result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtranscribe(audio_file, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m, fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m pred_text \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu-baru/lib/python3.12/site-packages/whisper/__init__.py:137\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[1;32m    134\u001b[0m     download_root \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXDG_CACHE_HOME\u001b[39m\u001b[38;5;124m\"\u001b[39m, default), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhisper\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _MODELS:\n\u001b[0;32m--> 137\u001b[0m     checkpoint_file \u001b[38;5;241m=\u001b[39m _download(_MODELS[name], download_root, in_memory)\n\u001b[1;32m    138\u001b[0m     alignment_heads \u001b[38;5;241m=\u001b[39m _ALIGNMENT_HEADS[name]\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(name):\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu-baru/lib/python3.12/site-packages/whisper/__init__.py:66\u001b[0m, in \u001b[0;36m_download\u001b[0;34m(url, root, in_memory)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(download_target, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     65\u001b[0m     model_bytes \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hashlib\u001b[38;5;241m.\u001b[39msha256(model_bytes)\u001b[38;5;241m.\u001b[39mhexdigest() \u001b[38;5;241m==\u001b[39m expected_sha256:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_bytes \u001b[38;5;28;01mif\u001b[39;00m in_memory \u001b[38;5;28;01melse\u001b[39;00m download_target\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "video_file = \"/content/interview_question_1.mp4\"\n",
    "audio_file = \"audio_1.wav\"\n",
    "os.system(f\"ffmpeg -i {video_file} -ar 16000 -ac 1 -vn {audio_file} -y\")\n",
    "\n",
    "ground_truth = \"\"\"Can you share any specific challenges you face while working on certification and how you overcome them? Ah, okay. Actually, for the challenges, there are some challenges when I took the certifications, especially for the projects I mentioned that I already working with it. The first one is actually to meet the specific accuracy or validation loss right for the evaluation matrix.\n",
    "And yeah, actually, that's just need to take some trial and error with different architecture. For example, we can try to add more layer, more neurons, changes the neurons, or even I also apply the dropout layer. So yeah, it really helps with the validation loss to become more lower, right? And yeah, I think that's one of the biggest challenges that I have while working on these certifications.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model_name = \"medium\"  # tiny/base/small/medium/large-v2\n",
    "model = whisper.load_model(model_name)\n",
    "\n",
    "result = model.transcribe(audio_file, language=\"en\", fp16=False)\n",
    "pred_text = result[\"text\"].strip()\n",
    "\n",
    "wer_score = wer(ground_truth.lower(), pred_text.lower())\n",
    "cer_score = cer(ground_truth.lower(), pred_text.lower())\n",
    "accuracy = max(0, (1 - wer_score) * 100)\n",
    "\n",
    "print(\"Predicted Text:\", pred_text)\n",
    "print(\"WER:\", round(wer_score, 4))\n",
    "print(\"CER:\", round(cer_score, 4))\n",
    "print(\"Accuracy (%):\", round(accuracy, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe2J-_yLN-vx"
   },
   "source": [
    "**VIDIO 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-17T14:33:26.543157Z",
     "iopub.status.busy": "2025-11-17T14:33:26.542283Z",
     "iopub.status.idle": "2025-11-17T14:33:42.361061Z",
     "shell.execute_reply": "2025-11-17T14:33:42.358560Z",
     "shell.execute_reply.started": "2025-11-17T14:33:26.543104Z"
    },
    "id": "7JOQEw-YN7rg",
    "outputId": "a0ba6b56-de79-49d8-8439-b75bd4512ed2"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 4. Pilih Model Whisper\u001b[39;00m\n\u001b[1;32m     12\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedium\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# bisa diganti tiny/base/small/medium/large-v2\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mload_model(model_name)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 5. Transcribe & Evaluasi\u001b[39;00m\n\u001b[1;32m     16\u001b[0m result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtranscribe(audio_file, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m, fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu-baru/lib/python3.12/site-packages/whisper/__init__.py:155\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m checkpoint_file\n\u001b[1;32m    154\u001b[0m dims \u001b[38;5;241m=\u001b[39m ModelDimensions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheckpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdims\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 155\u001b[0m model \u001b[38;5;241m=\u001b[39m Whisper(dims)\n\u001b[1;32m    156\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alignment_heads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu-baru/lib/python3.12/site-packages/whisper/model.py:263\u001b[0m, in \u001b[0;36mWhisper.__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims \u001b[38;5;241m=\u001b[39m dims\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m AudioEncoder(\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_mels,\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_audio_ctx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_audio_layer,\n\u001b[1;32m    262\u001b[0m )\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m TextDecoder(\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_vocab,\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_text_ctx,\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_text_state,\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_text_head,\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_text_layer,\n\u001b[1;32m    269\u001b[0m )\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# use the last half among the decoder layers for time alignment by default;\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# to use a specific set of heads, see `set_alignment_heads()` below.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m all_heads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_text_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_text_head, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool\n\u001b[1;32m    274\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu-baru/lib/python3.12/site-packages/whisper/model.py:218\u001b[0m, in \u001b[0;36mTextDecoder.__init__\u001b[0;34m(self, n_vocab, n_ctx, n_state, n_head, n_layer)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(n_vocab, n_state)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mempty(n_ctx, n_state))\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks: Iterable[ResidualAttentionBlock] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    217\u001b[0m     [\n\u001b[0;32m--> 218\u001b[0m         ResidualAttentionBlock(n_state, n_head, cross_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layer)\n\u001b[1;32m    220\u001b[0m     ]\n\u001b[1;32m    221\u001b[0m )\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln \u001b[38;5;241m=\u001b[39m LayerNorm(n_state)\n\u001b[1;32m    224\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(n_ctx, n_ctx)\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf)\u001b[38;5;241m.\u001b[39mtriu_(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu-baru/lib/python3.12/site-packages/whisper/model.py:146\u001b[0m, in \u001b[0;36mResidualAttentionBlock.__init__\u001b[0;34m(self, n_state, n_head, cross_attention)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_state: \u001b[38;5;28mint\u001b[39m, n_head: \u001b[38;5;28mint\u001b[39m, cross_attention: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;241m=\u001b[39m MultiHeadAttention(n_state, n_head)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_ln \u001b[38;5;241m=\u001b[39m LayerNorm(n_state)\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    150\u001b[0m         MultiHeadAttention(n_state, n_head) \u001b[38;5;28;01mif\u001b[39;00m cross_attention \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu-baru/lib/python3.12/site-packages/whisper/model.py:89\u001b[0m, in \u001b[0;36mMultiHeadAttention.__init__\u001b[0;34m(self, n_state, n_head)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery \u001b[38;5;241m=\u001b[39m Linear(n_state, n_state)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m Linear(n_state, n_state, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m Linear(n_state, n_state)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout \u001b[38;5;241m=\u001b[39m Linear(n_state, n_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu-baru/lib/python3.12/site-packages/torch/nn/modules/linear.py:115\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_parameters()\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu-baru/lib/python3.12/site-packages/torch/nn/modules/linear.py:124\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03mResets parameters based on their initialization used in ``__init__``.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m init\u001b[38;5;241m.\u001b[39mkaiming_uniform_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, a\u001b[38;5;241m=\u001b[39mmath\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu-baru/lib/python3.12/site-packages/torch/nn/init.py:573\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    571\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39muniform_(\u001b[38;5;241m-\u001b[39mbound, bound, generator\u001b[38;5;241m=\u001b[39mgenerator)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# video_file = \"/content/interview_question_2.webm\"\n",
    "audio_file = \"./tmp/audio/gdrive_video.16k.wav\"\n",
    "# os.system(f\"ffmpeg -i {video_file} -ar 16000 -ac 1 -vn {audio_file} -y\")\n",
    "\n",
    "# 3. Ground Truth\n",
    "ground_truth = \"\"\"Can you describe your experience with transfer learning and TensorFlow? How do you benefit from the projects? About transfer learning, we use existing trained models from TensorFlow, for example, like VGG-16, VGG-19, right? Especially for some cases that we need to use deep learning using Keras applications, for example, like image classification, we can use transfer learning models, which is already a trained model with exceptionally high accuracy, high performance. Even though it's trained with different datasets, but it really helps to improve our model performance, model accuracy, model loss. For example, like MobileNet, VGG-19, VGG-16, EfficientNet, it will help to improve our models comparing to the one if we use a traditional CNN model.\n",
    "CNN model with the convolutional 2D, max pooling, and it's quite good actually to use transfer learning. It really helps with our model performance, to improve our model performance.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 4. Pilih Model Whisper\n",
    "model_name = \"medium\"  # bisa diganti tiny/base/small/medium/large-v2\n",
    "model = whisper.load_model(model_name)\n",
    "\n",
    "# 5. Transcribe & Evaluasi\n",
    "result = model.transcribe(audio_file, language=\"en\", fp16=False)\n",
    "pred_text = result[\"text\"].strip()\n",
    "\n",
    "wer_score = wer(ground_truth.lower(), pred_text.lower())\n",
    "cer_score = cer(ground_truth.lower(), pred_text.lower())\n",
    "accuracy = max(0, (1 - wer_score) * 100)\n",
    "\n",
    "print(\"Predicted Text:\", pred_text)\n",
    "print(\"WER:\", round(wer_score, 4))\n",
    "print(\"CER:\", round(cer_score, 4))\n",
    "print(\"Accuracy (%):\", round(accuracy, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T14:33:47.125372Z",
     "iopub.status.busy": "2025-11-17T14:33:47.123920Z",
     "iopub.status.idle": "2025-11-17T14:33:47.136158Z",
     "shell.execute_reply": "2025-11-17T14:33:47.134006Z",
     "shell.execute_reply.started": "2025-11-17T14:33:47.125311Z"
    }
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "import numpy as np\n",
    "\n",
    "def transcribe_local(audio_path, model_path):\n",
    "    \"\"\"\n",
    "    Transkripsi audio menggunakan Whisper lokal (.pt) TIDAK memakai config atau downloader.\n",
    "    Mengembalikan: text, segments, meta\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Memuat Whisper dari lokal: {model_path}\")\n",
    "    model = whisper.load_model(model_path)\n",
    "\n",
    "    print(f\"Mulai transkripsi audio: {audio_path}\")\n",
    "    result = model.transcribe(audio_path, language=\"en\", fp16=False)\n",
    "\n",
    "    text = (result.get(\"text\") or \"\").strip()\n",
    "    segments = result.get(\"segments\", [])\n",
    "\n",
    "    # Meta yang sama seperti stt.py\n",
    "    if segments:\n",
    "        avg_logprob = float(np.mean([float(s.get(\"avg_logprob\", -1.0)) for s in segments]))\n",
    "        no_speech_prob = float(np.mean([float(s.get(\"no_speech_prob\", 0.0)) for s in segments]))\n",
    "        duration = float(segments[-1][\"end\"])\n",
    "    else:\n",
    "        avg_logprob = -1.0\n",
    "        no_speech_prob = 1.0\n",
    "        duration = 0.0\n",
    "\n",
    "    meta = {\n",
    "        \"avg_logprob\": avg_logprob,\n",
    "        \"no_speech_prob\": no_speech_prob,\n",
    "        \"duration_sec\": duration\n",
    "    }\n",
    "\n",
    "    return text, segments, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T14:33:51.167561Z",
     "iopub.status.busy": "2025-11-17T14:33:51.166975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat Whisper dari lokal: /home/umaygans/.cache/whisper/medium.pt\n",
      "Mulai transkripsi audio: /home/umaygans/Capstone-asah/experiments/gdrive_video.16k.wav\n"
     ]
    }
   ],
   "source": [
    "from jiwer import wer, cer\n",
    "\n",
    "# Lokasi audio hasil ffmpeg\n",
    "audio_file = \"/home/umaygans/Capstone-asah/experiments/gdrive_video.16k.wav\"\n",
    "\n",
    "# Lokasi Whisper Medium lokal\n",
    "model_path = \"/home/umaygans/.cache/whisper/medium.pt\"\n",
    "\n",
    "# Ground Truth\n",
    "ground_truth = \"\"\"Can you share any specific challenges you faced while working on certification and how you \n",
    "overcome them? Ah, okay actually, for the challenges, there are some challenges when I took the certifications, especially for the project submission that I already working with it, the first one is actually to meet the specific accuracy or validation loss for the evaluation matrix, and actually that just need to take some trial and error with different architecture, for example, we can try to add more layer, more neurons, changes the neurons, or even I also apply the dropout layer, so it really helps with the validation loss to become more lower, and I think that's one of the biggest challenges that I have while working on these certifications. Okay.\n",
    "\"\"\"\n",
    "\n",
    "# Transkripsi\n",
    "pred_text, segments, meta = transcribe_local(audio_file, model_path)\n",
    "\n",
    "# Evaluasi WER/CER\n",
    "wer_score = wer(ground_truth.lower(), pred_text.lower())\n",
    "cer_score = cer(ground_truth.lower(), pred_text.lower())\n",
    "accuracy = max(0, (1 - wer_score) * 100)\n",
    "\n",
    "print(\"Predicted Text:\", pred_text)\n",
    "print(\"Segments:\", len(segments))\n",
    "print(\"Meta:\", meta)\n",
    "print(\"WER:\", round(wer_score, 4))\n",
    "print(\"CER:\", round(cer_score, 4))\n",
    "print(\"Accuracy (%):\", round(accuracy, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWu6NyyMOCuj"
   },
   "source": [
    "**VIDIO 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ne14G46OFBJ",
    "outputId": "a10a0b77-a7ba-4560-83f5-6110e33a5d00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Text: Wait, what is this? Describe a complex TensorFlow model you have built and the steps you took to ensure its accuracy and efficiency. Hmm, complex TensorFlow model you have built and steps you took to ensure its accuracy. I will take one of my previous project that I used. I also used Keras TensorFlow model. It is about cellular disease prediction. I used this model for my undergraduate thesis for my script C. I used this model. It is quite challenging even though it has achieved high accuracy with some dense layer, with some throughout layer, and trial and error with the callback function, with the neurons. But the problem is the dataset is not balanced. It has the in-balance class datasets. The approach that I used is just to use the technique called smooth and synthetic oversampling technique with edited nearest neighbor. Basically it is just oversampling and undersampling the datasets. It helps with the accuracy.\n",
      "WER: 0.2038\n",
      "CER: 0.0814\n",
      "Accuracy (%): 79.62\n"
     ]
    }
   ],
   "source": [
    "\n",
    "video_file = \"/content/interview_question_3.webm\"\n",
    "audio_file = \"audio_3.wav\"\n",
    "os.system(f\"ffmpeg -i {video_file} -ar 16000 -ac 1 -vn {audio_file} -y\")\n",
    "\n",
    "# 3. Ground Truth\n",
    "ground_truth = \"\"\"Wait, what is this? Describe a complex TensorFlow model you've built and the steps you took to ensure its accuracy and efficiency. Hmm, a complex TensorFlow model you've built and the steps you took to ensure its accuracy. Okay, I will take one of my previous project that I used.\n",
    "I also used Keras TensorFlow model. It is about celiac disease prediction. This is also I used the research project for my undergraduate thesis, for my script C. I used this model.\n",
    "It's quite challenging even though it's achieved high accuracy with some dense layer, with some drawout layer, and trial and error also with the callback function, with the neurons. But the problem is the dataset is not balanced. It has the imbalanced class datasets.\n",
    "The approach that I used is just to use the technique called smooth N, synthetic oversampling technique, with edited nearest neighbor. Basically, it's just oversampling and undersampling the datasets. It helps with the accuracy.\n",
    "\"\"\"\n",
    "\n",
    "# 4. Pilih Model Whisper\n",
    "model_name = \"medium\"  # bisa diganti tiny/base/small/medium/large-v2\n",
    "model = whisper.load_model(model_name)\n",
    "\n",
    "# 5. Transcribe & Evaluasi\n",
    "result = model.transcribe(audio_file, language=\"en\", fp16=False)\n",
    "pred_text = result[\"text\"].strip()\n",
    "\n",
    "wer_score = wer(ground_truth.lower(), pred_text.lower())\n",
    "cer_score = cer(ground_truth.lower(), pred_text.lower())\n",
    "accuracy = max(0, (1 - wer_score) * 100)\n",
    "\n",
    "print(\"Predicted Text:\", pred_text)\n",
    "print(\"WER:\", round(wer_score, 4))\n",
    "print(\"CER:\", round(cer_score, 4))\n",
    "print(\"Accuracy (%):\", round(accuracy, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DciOBsrPOHk7"
   },
   "source": [
    "**VIDIO 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FN7nSkkIOJLc",
    "outputId": "9343e342-55fc-4d44-a42f-baf0b2ced6e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Text: Explain how you implement dropout in test server model and test on training. Previously, I also have implemented the dropout layer also in the project solution within this certification. We can just add the dropout layer, for example, if I'm not mistaken, I have used this dropout layer in the one that the case is image classification, German traffic something if I'm not wrong. I've used this dropout layer in the middle of the layer. So there's a flattened layer, right? Not flattened, the convolutional layer and then the flattened layer. And I use that dropout layer, which is I use with the rate of 0.2 or 0.5 if I'm not wrong. And then the dense layer and the last the output layer, right? The effect is it will really helps to improve our accuracy and lower our validation loss by turning off some of the previous layer. For example, like we have dense layer 64 and the next layer, we implement the dropout layer with the rate of 0.5 and it will turn off randomly each epoch of the previous dense layer.\n",
      "WER: 0.2616\n",
      "CER: 0.1191\n",
      "Accuracy (%): 73.84\n"
     ]
    }
   ],
   "source": [
    "\n",
    "video_file = \"/content/interview_question_4.webm\"\n",
    "audio_file = \"audio_4.wav\"\n",
    "os.system(f\"ffmpeg -i {video_file} -ar 16000 -ac 1 -vn {audio_file} -y\")\n",
    "\n",
    "# 3. Ground Truth\n",
    "ground_truth = \"\"\"Explain how to implement dropout in TensorFlow model and the effect it has on training. Previously, I also have implemented the dropout layer, also in the project solution, within these certifications. We can just add the dropout layer, for example, if I'm not mistaken, I have used this dropout layer in the image classifications, German traffic something, if I'm not wrong.\n",
    "I have used this dropout layer in the middle of the layer, so there is a flattened layer, not flattened, the convolutional layer, another flattened layer, and I used that dropout layer, which I used with the rate of 0.2 or 0.5, if I'm not wrong, and then the dense layer and the output layer. The effect is, it will help to improve our accuracy and lower our validation loss by turning off some of the previous layers. For example, we have dense layer 64, and the next layer, we implement the dropout layer with the rate of 0.5, and it will turn off randomly each epoch of the previous dense layer.\n",
    "\"\"\"\n",
    "\n",
    "# 4. Pilih Model Whisper\n",
    "model_name = \"medium\"  # bisa diganti tiny/base/small/medium/large-v2\n",
    "model = whisper.load_model(model_name)\n",
    "\n",
    "# 5. Transcribe & Evaluasi\n",
    "result = model.transcribe(audio_file, language=\"en\", fp16=False)\n",
    "pred_text = result[\"text\"].strip()\n",
    "\n",
    "wer_score = wer(ground_truth.lower(), pred_text.lower())\n",
    "cer_score = cer(ground_truth.lower(), pred_text.lower())\n",
    "accuracy = max(0, (1 - wer_score) * 100)\n",
    "\n",
    "print(\"Predicted Text:\", pred_text)\n",
    "print(\"WER:\", round(wer_score, 4))\n",
    "print(\"CER:\", round(cer_score, 4))\n",
    "print(\"Accuracy (%):\", round(accuracy, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BerXDa-HOLiy"
   },
   "source": [
    "VIDIO 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "honpjhbkONIW",
    "outputId": "6d68539e-6e92-46de-cf46-21127d1f2979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Text: Describe the process of building more confusing words for image as fiction. Okay, the CNN run, right? So, at the first time, of course, we need to make sure there are split, the image folder is split for each class. And then we can use Keras per Processing, if I'm not mistaken, image dataset from directory to split the training and validation dataset. Of course, we can use another set, which is the test dataset. Okay, the next one, we can just, maybe we need to implement also the image data augmentation tool to make our dataset more veritable. For example, we can rotate, we can zoom it, we can crop it. And the last thing, of course, we can build our model with the conventional 2D, specify the filters, the kernel size, the activation of course, the input shape for the first layer. And then we can apply the MaxPulling2D and the next layer, we can just use Conversion 2D, MaxPulling and whatever it is. And after that, we apply the Flatten layer and Dropout layer if you want. And the last thing, don't forget to use the Dance Layer for the output.\n",
      "WER: 0.1959\n",
      "CER: 0.1197\n",
      "Accuracy (%): 80.41\n"
     ]
    }
   ],
   "source": [
    "\n",
    "video_file = \"/content/interview_question_5.webm\"\n",
    "audio_file = \"audio_5.wav\"\n",
    "os.system(f\"ffmpeg -i {video_file} -ar 16000 -ac 1 -vn {audio_file} -y\")\n",
    "\n",
    "# 3. Ground Truth\n",
    "ground_truth = \"\"\"Describe the process of building or configuring your image folder for image restriction. Okay, the CNN one, right? So, at the first time, of course, we need to make sure the image folder is split for each class. And then we can use Keras preprocessing, if I'm not mistaken, image dataset from directory to split the training and the validation dataset.\n",
    "Of course, we can use another set, which is the test dataset.  Okay, the next one, we can just, maybe we need to implement also the image augmentation, data image augmentation to make our dataset more verity. For example, we can rotate, we can zoom it, we can crop it. And the last thing, of course, we can build our chain model with the convolutional 2D, specify the filters, the kernel size, the error activation, of course, the input shape for the first layer. And then we can apply the mesh pooling 2D.  And the next layer, we can just use convolutional 2D, mesh pooling, and whatever it is.\n",
    "And after that, we apply the flatten layer and dropout layer, if you want. And the last thing, don't forget to use the dense layer, right, for the output.\"\"\"\n",
    "\n",
    "# 4. Pilih Model Whisper\n",
    "model_name = \"medium\"  # bisa diganti tiny/base/small/medium/large-v2\n",
    "model = whisper.load_model(model_name)\n",
    "\n",
    "# 5. Transcribe & Evaluasi\n",
    "result = model.transcribe(audio_file, language=\"en\", fp16=False)\n",
    "pred_text = result[\"text\"].strip()\n",
    "\n",
    "wer_score = wer(ground_truth.lower(), pred_text.lower())\n",
    "cer_score = cer(ground_truth.lower(), pred_text.lower())\n",
    "accuracy = max(0, (1 - wer_score) * 100)\n",
    "\n",
    "print(\"Predicted Text:\", pred_text)\n",
    "print(\"WER:\", round(wer_score, 4))\n",
    "print(\"CER:\", round(cer_score, 4))\n",
    "print(\"Accuracy (%):\", round(accuracy, 2))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "GPU baru",
   "language": "python",
   "name": "gpu-baru"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
